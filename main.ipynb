{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTS - Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ljspeech import LJSPEECH\n",
    "\n",
    "BLOCK_SIZE        = 512\n",
    "DATASET_PATH      = \"./data/LJSpeech/\"\n",
    "BANDWIDTH_IDX     = 0\n",
    "BANDWIDTHS        = [1.5, 3.0, 6.0, 12.0, 24.0]\n",
    "BANDWIDTH         = BANDWIDTHS[BANDWIDTH_IDX]\n",
    "MAX_PROMPT_LENGTH = 128\n",
    "\n",
    "dataset = LJSPEECH(\"./data/LJSpeech\",\n",
    "                    encodec_bandwidth=BANDWIDTH,\n",
    "                    max_prompt_length=MAX_PROMPT_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, n_embed, BLOCK_SIZE, dropout):\n",
    "        super().__init__()\n",
    "        self.k = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.q = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.v = nn.Linear(n_embed, head_size, bias=False)\n",
    "\n",
    "        self.register_buffer(\"tril\",\n",
    "                             torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.k(x)\n",
    "        q = self.q(x)\n",
    "        w = q @ k.transpose(-2, -1) * C ** -0.5\n",
    "        w = w.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "        v = self.v(x)\n",
    "        o = w @ v\n",
    "        return o\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_embed, num_heads, head_size, dropout, BLOCK_SIZE):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Head(head_size, n_embed, BLOCK_SIZE, dropout)\n",
    "             for _ in range(num_heads)])\n",
    "        self.proj  = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # print(\"MHSA proj.shape:\", n_embed)\n",
    "    def forward(self, x):\n",
    "        # print(\"MHSA x.shape:\", x.shape)\n",
    "        o = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # print(\"MHSA concat o.shape:\", o.shape)\n",
    "        o = self.dropout(self.proj(o))\n",
    "        # print(\"MHSA project o.shape:\", o.shape)\n",
    "        return o\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, dropout, BLOCK_SIZE):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa   = MultiHeadAttention(n_embed, n_head, head_size, dropout, BLOCK_SIZE)\n",
    "        self.ffwd = FeedForward(n_embed, dropout)\n",
    "        self.ln1  = nn.LayerNorm(n_embed)\n",
    "        self.ln2  = nn.LayerNorm(n_embed)\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_len, n_embed, n_heads, n_layer, BLOCK_SIZE, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.BLOCK_SIZE = BLOCK_SIZE\n",
    "        self.token_emb_table    = nn.Embedding(vocab_len, n_embed)\n",
    "        self.position_emb_table = nn.Embedding(BLOCK_SIZE, n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embed, n_head=n_heads, dropout=dropout, BLOCK_SIZE=BLOCK_SIZE)\n",
    "              for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_len)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_embed = self.token_emb_table(idx)\n",
    "        pos_embed = self.position_emb_table(\n",
    "            torch.arange(T, device=\"cpu\"))\n",
    "\n",
    "        x = tok_embed + pos_embed\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if not targets is None:\n",
    "            B, T, C = logits.shape\n",
    "            logits  = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.BLOCK_SIZE:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "vocab_len = 1024 + 1 + len(dataset.phone_dict)\n",
    "# vocab_len = 1024\n",
    "model = TransformerDecoder(\n",
    "    vocab_len=vocab_len,\n",
    "    n_embed=256,\n",
    "    n_heads=4,\n",
    "    n_layer=1,\n",
    "    BLOCK_SIZE=BLOCK_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('data/LJSpeech/LJSpeech-1.1/wavs/LJ001-0002.wav'),\n",
       " tensor([[-0.0003,  0.0000,  0.0000,  ..., -0.0009, -0.0010, -0.0011]]),\n",
       " 22050,\n",
       " 'in being comparatively modern.',\n",
       " 'in being comparatively modern.',\n",
       " ['IH0',\n",
       "  'N',\n",
       "  '_',\n",
       "  'B',\n",
       "  'IY1',\n",
       "  'IH0',\n",
       "  'NG',\n",
       "  '_',\n",
       "  'K',\n",
       "  'AH0',\n",
       "  'M',\n",
       "  'P',\n",
       "  'EH1',\n",
       "  'R',\n",
       "  'AH0',\n",
       "  'T',\n",
       "  'IH0',\n",
       "  'V',\n",
       "  'L',\n",
       "  'IY0',\n",
       "  '_',\n",
       "  'M',\n",
       "  'AA1',\n",
       "  'D',\n",
       "  'ER0',\n",
       "  'N',\n",
       "  '_',\n",
       "  '_'],\n",
       " tensor([38, 48, 74, 22, 42, 38, 49, 74, 45, 10, 47, 56, 27, 57, 10, 60, 38, 69,\n",
       "         46, 41, 74, 47,  5, 24, 29, 48, 74, 74]),\n",
       " tensor([[[ 738,  523,  141,  504,  970,  363,  746,  913,  949, 1010,  530,\n",
       "            347,  860,  319,  477,  840,  801,  319,  765,  465,  727,  727,\n",
       "            906,  840,  990,  801,  765,  563,  807,  565,   25,  276,  904,\n",
       "            194,  935,  779,  283,  913,  945,  563,  807,  976,  404,   52,\n",
       "            325,  904, 1020,  666,  372,  677,  537,  695,  352,  348,  240,\n",
       "            222,  612,  734,  950,  734,  451,  694,  288,   82,  694,  407,\n",
       "             23,  106,   73,  887,  887,  619,  148, 1001,  884,  588,  612,\n",
       "            890,  432,  604,  819,  176,  148,  676,  182,  944,  934,  944,\n",
       "            944,  288,  956,  407,  465,  407,  465,  321,  143,  321,  860,\n",
       "            424,  598,  530,  860,  598,  421,   20,   20,   20,  793,  421,\n",
       "            203, 1009,  128,  695,  501,  222,  451,  971,   73,  838,   43,\n",
       "            942,  925,  690,  690,  871,  598,  904,  860,  699,  699,  834,\n",
       "            699,  834,  106,  430, 1017,  738,  106,  408,  738,  408,  738],\n",
       "          [ 868,  414, 1020,  662,  949,  996,  679,  897,  603,  833,  896,\n",
       "            942,  975,  801,  711,  662,  662,  193,  193,  334,  662,  268,\n",
       "            856,  193,  662,  212,  471,  719,  993,  812,  560,  222,  870,\n",
       "            713,  988,  580,  568,  327,  673,  272,  222,  560,  673,  673,\n",
       "            643,  205,  791,  833,  481,  481,  384, 1009,  801,  502,   14,\n",
       "            486,  626,  182,  486,  634,  948,  174,  357,  702,  457,  471,\n",
       "            471,  580,  948,  601,  205,  739,   92,  899,  725,  268,   52,\n",
       "            512,  350,  102,  430,  700,  580,  577,  822,  214,  444,  214,\n",
       "            739,  517,    0,  372,  268,  420,  653,  743,  176,  964,  708,\n",
       "            831,  298,  827,  425,  425,  812,  577,  577,  577,  192,  298,\n",
       "            568,  546,  444,  851,  708,  174,  252,  805,  497,  308,  182,\n",
       "            390,  752,  948,  815,  371,  990,   71, 1007,  787,  960,  363,\n",
       "            516,  765,  765,  483,  646,  518,  913,  424,  544,  424,  544]]]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 134])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_item = dataset[4][-1]\n",
    "org_item.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = rearrange(org_item, \"b q t -> b (t q)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/96zdxlqn75533pl5gf29f3bw0000gn/T/ipykernel_10563/472372582.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = torch.tensor(item, device=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "item = torch.tensor(item, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = torch.clamp(item, 0, 1023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 268])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss = model(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.2993,  0.2959, -0.3080,  ...,  0.5133, -0.7259, -0.0948],\n",
       "          [-0.1246,  0.2828,  0.0867,  ..., -0.7155,  0.3794,  1.1395],\n",
       "          [-0.2055, -1.0728,  0.2162,  ...,  0.1552,  1.1568,  0.1658],\n",
       "          ...,\n",
       "          [ 0.1314,  0.2591,  0.6565,  ...,  0.1458,  0.2549,  0.5848],\n",
       "          [-1.0401, -0.4139, -0.6558,  ...,  0.4244,  0.2001,  0.9856],\n",
       "          [-0.1315, -0.0691, -0.4677,  ...,  0.8147, -0.8719, -0.3951]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 276,  609,  807,  478,  112,  727,  658,  836,  575,  460,  942,  422,\n",
       "         942,  460,  160, 1010,   47, 1010,   47,  420,   47,  973,   47,  742,\n",
       "         160,  742,  339,  752,  583,  672,  868,  315,  784,  227,  987,  185,\n",
       "         984,  952,  998,  985,  333,  792,  841,  869,  548,  881,  548,  869,\n",
       "         514,  469,  375,  469,  695,  276,  432,  722,   73,  829,  251,  177,\n",
       "         759,    6,   43,  345,  656,  855,  743,  959,   43,  858,  808,   89,\n",
       "          43,  320,  808,  227,  699, 1002,  967, 1002, 1011,  646,  457,  646,\n",
       "         604,  230,  602,  721,  980,  536,  920,  711,  356,  792,  939,  711,\n",
       "         584,  711,  796,  869,  833,  869,  699,  133,  321,  556,  457,  418,\n",
       "         136,   31,  676,  102,   47,  420,  744, 1010,   47,  973,  574,  160,\n",
       "         160,  549,  160,  857,  148,  993,  148,  443,  103,  336,  276, 1010,\n",
       "         264,  615,  197,  615,  833,  413,  182,  920,  862,  541, 1019,  857,\n",
       "         855,   71,  855,  888,  855,  601,  106, 1007,  335,  713,  276,  481,\n",
       "         677,  734,  887,  974,  967,  266,  421,  180,   99,  727,  793,  932,\n",
       "         421,  727,  456,  194,  385,  803,  385,  244,  421,  678,  385,  194,\n",
       "         385,  798,  747,  526,  240,  826,  162,  460,  747,  460,  275,  122,\n",
       "         402,  463,  275,  505,  977,  527,  696,  463,  275,  221,  565,  228,\n",
       "         753,  460,  753,  460,  887,  700,  160,  973,  148,  794,  160,  857,\n",
       "         160, 1010,  160, 1010,  160,  646,  160,  857,  160,  875,  160,  196,\n",
       "         160,  857,  160,  196,  160,  196,  160,  652,  876,  652,  339,  913,\n",
       "         339,  857,  339,  857,  940,  974,  237,  957,   25,  420,  106,  870,\n",
       "         408,  544,  835,  518,  835,  913,  738,  544,  835,  913,  106,  913,\n",
       "         408,  424,  738,  913])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = item[0, 0:-2].unsqueeze(0)\n",
    "new_list = torch.tensor([item[0, 1:-1].tolist()], dtype=torch.long)\n",
    "yb = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 266]), torch.Size([1, 266]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(7.2001, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(3.5264, grad_fn=<NllLossBackward0>)\n",
      "20 tensor(0.9719, grad_fn=<NllLossBackward0>)\n",
      "30 tensor(0.1463, grad_fn=<NllLossBackward0>)\n",
      "40 tensor(0.0374, grad_fn=<NllLossBackward0>)\n",
      "50 tensor(0.0188, grad_fn=<NllLossBackward0>)\n",
      "60 tensor(0.0128, grad_fn=<NllLossBackward0>)\n",
      "70 tensor(0.0105, grad_fn=<NllLossBackward0>)\n",
      "80 tensor(0.0090, grad_fn=<NllLossBackward0>)\n",
      "90 tensor(0.0081, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPOCHS):\n",
    "    optim.zero_grad()\n",
    "    _, loss = model(xb, targets=yb)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if e % 10 == 0:\n",
    "        print(e, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encodec_util import decode_to_file\n",
    "pred = model.generate(torch.zeros((1, 1), dtype=torch.int), max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 101])\n"
     ]
    }
   ],
   "source": [
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_pred = pred[0, :pred.shape[1] - (pred.shape[1] % 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clipped_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pred = rearrange(clipped_pred.squeeze(0), \"(t q) -> t q\", q=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,  609],\n",
       "        [ 807,  478],\n",
       "        [ 112,  727],\n",
       "        [ 658,  836],\n",
       "        [ 575,  460],\n",
       "        [ 942,  422],\n",
       "        [ 942,  460],\n",
       "        [ 160, 1010],\n",
       "        [  47, 1010],\n",
       "        [  47,  420],\n",
       "        [  47,  973],\n",
       "        [  47,  742],\n",
       "        [ 160,  742],\n",
       "        [ 339,  752],\n",
       "        [ 583,  672],\n",
       "        [ 868,  315],\n",
       "        [ 784,  227],\n",
       "        [ 987,  185],\n",
       "        [ 984,  952],\n",
       "        [ 998,  985],\n",
       "        [ 333,  792],\n",
       "        [ 841,  869],\n",
       "        [ 548,  881],\n",
       "        [ 548,  869],\n",
       "        [ 514,  469],\n",
       "        [ 375,  469],\n",
       "        [ 695,  276],\n",
       "        [ 432,  722],\n",
       "        [  73,  829],\n",
       "        [ 251,  177],\n",
       "        [ 759,    6],\n",
       "        [  43,  345],\n",
       "        [ 656,  855],\n",
       "        [ 743,  959],\n",
       "        [  43,  858],\n",
       "        [ 808,   89],\n",
       "        [  43,  320],\n",
       "        [ 808,  227],\n",
       "        [ 699, 1002],\n",
       "        [ 967, 1002],\n",
       "        [1011,  646],\n",
       "        [ 457,  646],\n",
       "        [ 604,  230],\n",
       "        [ 602,  721],\n",
       "        [ 980,  536],\n",
       "        [ 920,  711],\n",
       "        [ 356,  792],\n",
       "        [ 939,  711],\n",
       "        [ 584,  711],\n",
       "        [ 796,  869]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor(1011))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(out_pred), torch.max(out_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 2])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_item_test = rearrange(org_item.squeeze(0), \"q t -> t q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pred = torch.clamp(out_pred, 1, 1023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_to_file(out_pred, \"out.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
